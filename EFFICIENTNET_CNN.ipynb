{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "usual-storm",
   "metadata": {
    "papermill": {
     "duration": 0.019383,
     "end_time": "2021-09-29T14:24:47.345321",
     "exception": false,
     "start_time": "2021-09-29T14:24:47.325938",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Basically a 1D CNN starter with bandpass. Filter size hard-coded from [https://www.kaggle.com/kit716/grav-wave-detection](https://www.kaggle.com/kit716/grav-wave-detection) which uses the simple architecture from https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103 \n",
    "- Added inference to @hidehisaarai1213 's PyTorch starter, iteration order changed from Y.Nakama's pipeline: \"iter on loader first then load model\" to \"load model first then iter the loader\"\n",
    "- Version 3: average pool+ELU\n",
    "- Version 4: max pool+SiLU\n",
    "- Version 5: Generalized Mean pooling: a trainable L^p mean per channel (using ideas from Lebesgue measurable spaces) pooling added per the comments from @hannes82:\n",
    "   $$\\textbf{e} = \\left[\\left(\\frac{1}{|\\Omega|}\\sum_{u\\in{\\Omega}}x^{p}_{cu}\\right)^{\\frac{1}{p}}\\right]_{c=1,\\cdots,C} $$\n",
    "\n",
    "## Reference\n",
    "- pipeline: [Y.Nakama's notebook](https://www.kaggle.com/yasufuminakama/g2net-efficientnet-b7-baseline-training).\n",
    "- dataset: @hidehisaarai1213 https://www.kaggle.com/hidehisaarai1213/g2net-read-from-tfrecord-train-with-pytorch\n",
    "- 1d CNN modified from https://www.kaggle.com/kit716/grav-wave-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-alarm",
   "metadata": {
    "papermill": {
     "duration": 0.017809,
     "end_time": "2021-09-29T14:24:47.381539",
     "exception": false,
     "start_time": "2021-09-29T14:24:47.363730",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "occupational-sunday",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T14:24:47.428976Z",
     "iopub.status.busy": "2021-09-29T14:24:47.428302Z",
     "iopub.status.idle": "2021-09-29T14:24:53.988238Z",
     "shell.execute_reply": "2021-09-29T14:24:53.987661Z",
     "shell.execute_reply.started": "2021-09-18T04:53:12.680234Z"
    },
    "papermill": {
     "duration": 6.58891,
     "end_time": "2021-09-29T14:24:53.988417",
     "exception": false,
     "start_time": "2021-09-29T14:24:47.399507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "import tensorflow as tf  # for reading TFRecord Dataset\n",
    "import tensorflow_datasets as tfds  # for making tf.data.Dataset to return numpy arrays\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "executed-productivity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T14:24:54.069658Z",
     "iopub.status.busy": "2021-09-29T14:24:54.069092Z",
     "iopub.status.idle": "2021-09-29T14:24:54.072946Z",
     "shell.execute_reply": "2021-09-29T14:24:54.072542Z",
     "shell.execute_reply.started": "2021-09-18T04:53:19.273097Z"
    },
    "papermill": {
     "duration": 0.066273,
     "end_time": "2021-09-29T14:24:54.073061",
     "exception": false,
     "start_time": "2021-09-29T14:24:54.006788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAVEDIR = Path(\"./\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-retreat",
   "metadata": {
    "papermill": {
     "duration": 0.017905,
     "end_time": "2021-09-29T14:24:54.109073",
     "exception": false,
     "start_time": "2021-09-29T14:24:54.091168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "individual-christopher",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T14:24:54.151518Z",
     "iopub.status.busy": "2021-09-29T14:24:54.150280Z",
     "iopub.status.idle": "2021-09-29T14:24:54.153131Z",
     "shell.execute_reply": "2021-09-29T14:24:54.152640Z",
     "shell.execute_reply.started": "2021-09-18T04:55:10.240254Z"
    },
    "papermill": {
     "duration": 0.026249,
     "end_time": "2021-09-29T14:24:54.153230",
     "exception": false,
     "start_time": "2021-09-29T14:24:54.126981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    print_freq = 2500\n",
    "    num_workers = 4\n",
    "    scheduler = \"CosineAnnealingLR\"\n",
    "#     scheduler = 'ReduceLROnPlateau'\n",
    "    model_name = \"1dcnn\"\n",
    "    epochs = 4\n",
    "    T_max = 5\n",
    "    lr = 1e-4\n",
    "    min_lr = 1e-7\n",
    "    batch_size = 50\n",
    "    val_batch_size = 100\n",
    "    weight_decay = 1e-5\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1000\n",
    "    factor = 0.2\n",
    "    patience = 1\n",
    "    eps = 1e-7\n",
    "    seed = 1127802825\n",
    "    target_size = 1\n",
    "    target_col = \"target\"\n",
    "    n_fold = 5\n",
    "    trn_fold = [1, 3]  # [0, 1, 2, 3, 4]\n",
    "    train = True\n",
    "    bandpass_params = dict(lf=25, \n",
    "                           hf=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-yesterday",
   "metadata": {
    "papermill": {
     "duration": 0.017761,
     "end_time": "2021-09-29T14:24:54.189065",
     "exception": false,
     "start_time": "2021-09-29T14:24:54.171304",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "touched-installation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T14:24:54.232190Z",
     "iopub.status.busy": "2021-09-29T14:24:54.231446Z",
     "iopub.status.idle": "2021-09-29T14:24:54.237586Z",
     "shell.execute_reply": "2021-09-29T14:24:54.237114Z",
     "shell.execute_reply.started": "2021-09-18T04:53:19.329762Z"
    },
    "papermill": {
     "duration": 0.030623,
     "end_time": "2021-09-29T14:24:54.237688",
     "exception": false,
     "start_time": "2021-09-29T14:24:54.207065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    score = roc_auc_score(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "def init_logger(log_file=SAVEDIR / 'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-fields",
   "metadata": {
    "papermill": {
     "duration": 0.018028,
     "end_time": "2021-09-29T14:24:54.274094",
     "exception": false,
     "start_time": "2021-09-29T14:24:54.256066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TFRecord Loader\n",
    "\n",
    "This is the heart of this notebook. Instead of using PyTorch's Dataset and DataLoader, here I define custom Loader that reads samples from TFRecords.\n",
    "\n",
    "FYI, there's a library that does the same thing, but its implementation is not optimized, so it's slower.\n",
    "\n",
    "https://github.com/vahidk/tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "anonymous-nightlife",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T14:24:54.322857Z",
     "iopub.status.busy": "2021-09-29T14:24:54.322245Z",
     "iopub.status.idle": "2021-09-29T14:25:03.448633Z",
     "shell.execute_reply": "2021-09-29T14:25:03.447866Z",
     "shell.execute_reply.started": "2021-09-18T04:53:19.343017Z"
    },
    "papermill": {
     "duration": 9.156785,
     "end_time": "2021-09-29T14:25:03.448770",
     "exception": false,
     "start_time": "2021-09-29T14:24:54.291985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://kds-b91baff186e64a73e9e78b57d377e1da69d2ac8d31dae63a4eef0505\n",
      "gs://kds-b19b8c2970ce9f1a4c603d222666fb9cf6ef93f0de9d2d44c7cb454b\n",
      "gs://kds-70ee6bbc669583514403707ddd5b73ff4c694827c630b313e9b92c05\n",
      "gs://kds-1a2860579359f728b709333a872776d27ab2d89dd7022cc3f3c0cdac\n",
      "train_files:  20\n"
     ]
    }
   ],
   "source": [
    "gcs_paths = []\n",
    "for i, j in [(0, 4), (5, 9), (10, 14), (15, 19)]:\n",
    "    path = f\"g2net-waveform-tfrecords-train-{i}-{j}\"\n",
    "    n_trial = 0\n",
    "    while True:\n",
    "        try:\n",
    "            gcs_path = KaggleDatasets().get_gcs_path(path)\n",
    "            gcs_paths.append(gcs_path)\n",
    "            print(gcs_path)\n",
    "            break\n",
    "        except:\n",
    "            if n_trial > 10:\n",
    "                break\n",
    "            n_trial += 1\n",
    "            continue\n",
    "            \n",
    "all_files = []\n",
    "for path in gcs_paths:\n",
    "    all_files.extend(np.sort(np.array(tf.io.gfile.glob(path + \"/train*.tfrecords\"))))\n",
    "    \n",
    "print(\"train_files: \", len(all_files))\n",
    "all_files = np.array(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "romance-minister",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T14:25:03.492700Z",
     "iopub.status.busy": "2021-09-29T14:25:03.492140Z",
     "iopub.status.idle": "2021-09-29T14:25:03.495489Z",
     "shell.execute_reply": "2021-09-29T14:25:03.495882Z",
     "shell.execute_reply.started": "2021-09-18T04:54:41.249849Z"
    },
    "papermill": {
     "duration": 0.027251,
     "end_time": "2021-09-29T14:25:03.495998",
     "exception": false,
     "start_time": "2021-09-29T14:25:03.468747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_data_items(fileids, train=True):\n",
    "    \"\"\"\n",
    "    Count the number of samples.\n",
    "    Each of the TFRecord datasets is designed to contain 28000 samples for train\n",
    "    22500 for test.\n",
    "    \"\"\"\n",
    "    sizes = 28000 if train else 22500\n",
    "    return len(fileids) * sizes\n",
    "\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-spokesman",
   "metadata": {
    "papermill": {
     "duration": 0.019388,
     "end_time": "2021-09-29T14:25:03.535252",
     "exception": false,
     "start_time": "2021-09-29T14:25:03.515864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bandpass\n",
    "\n",
    "Modified from various notebooks and https://www.kaggle.com/c/g2net-gravitational-wave-detection/discussion/261721#1458564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "innocent-ceramic",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T14:25:03.582918Z",
     "iopub.status.busy": "2021-09-29T14:25:03.581674Z",
     "iopub.status.idle": "2021-09-29T14:25:03.584475Z",
     "shell.execute_reply": "2021-09-29T14:25:03.584059Z",
     "shell.execute_reply.started": "2021-09-18T04:54:41.876474Z"
    },
    "papermill": {
     "duration": 0.02969,
     "end_time": "2021-09-29T14:25:03.584577",
     "exception": false,
     "start_time": "2021-09-29T14:25:03.554887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bandpass(x, lf=20, hf=500, order=8, sr=2048):\n",
    "    '''\n",
    "    Cell 33 of https://www.gw-openscience.org/LVT151012data/LOSC_Event_tutorial_LVT151012.html\n",
    "    https://scipy-cookbook.readthedocs.io/items/ButterworthBandpass.html\n",
    "    '''\n",
    "    sos = signal.butter(order, [lf, hf], btype=\"bandpass\", output=\"sos\", fs=sr)\n",
    "    normalization = np.sqrt((hf - lf) / (sr / 2))\n",
    "    window = signal.tukey(4096, 0.1)\n",
    "    if x.ndim ==2:\n",
    "        x *= window\n",
    "        for i in range(3):\n",
    "            x[i] = signal.sosfilt(sos, x[i]) * normalization\n",
    "    elif x.ndim == 3: # batch\n",
    "        for i in range(x.shape[0]):\n",
    "            x[i] *= window\n",
    "            for j in range(3):\n",
    "                x[i, j] = signal.sosfilt(sos, x[i, j]) * normalization\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "instrumental-milwaukee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T14:25:03.636468Z",
     "iopub.status.busy": "2021-09-29T14:25:03.635811Z",
     "iopub.status.idle": "2021-09-29T14:25:03.638231Z",
     "shell.execute_reply": "2021-09-29T14:25:03.638611Z",
     "shell.execute_reply.started": "2021-09-18T04:54:42.282028Z"
    },
    "papermill": {
     "duration": 0.034354,
     "end_time": "2021-09-29T14:25:03.638726",
     "exception": false,
     "start_time": "2021-09-29T14:25:03.604372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_wave(wave):\n",
    "    wave = tf.reshape(tf.io.decode_raw(wave, tf.float64), (3, 4096))\n",
    "    normalized_waves = []\n",
    "    scaling = tf.constant([1.5e-20, 1.5e-20, 0.5e-20], dtype=tf.float64)\n",
    "    for i in range(3):\n",
    "#         normalized_wave = wave[i] / tf.math.reduce_max(wave[i])\n",
    "        normalized_wave = wave[i] / scaling[i]\n",
    "        normalized_waves.append(normalized_wave)\n",
    "    wave = tf.stack(normalized_waves, axis=0)\n",
    "    wave = tf.cast(wave, tf.float32)\n",
    "    return wave\n",
    "\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    tfrec_format = {\n",
    "        \"wave\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"wave_id\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return prepare_wave(example[\"wave\"]), tf.reshape(tf.cast(example[\"target\"], tf.float32), [1]), example[\"wave_id\"]\n",
    "\n",
    "\n",
    "def read_unlabeled_tfrecord(example, return_image_id):\n",
    "    tfrec_format = {\n",
    "        \"wave\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"wave_id\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return prepare_wave(example[\"wave\"]), example[\"wave_id\"] if return_image_id else 0\n",
    "\n",
    "\n",
    "def get_dataset(files, batch_size=16, repeat=False, cache=False, \n",
    "                shuffle=False, labeled=True, return_image_ids=True):\n",
    "    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO, compression_type=\"GZIP\")\n",
    "    if cache:\n",
    "        # You'll need around 15GB RAM if you'd like to cache val dataset, and 50~60GB RAM for train dataset.\n",
    "        ds = ds.cache()\n",
    "\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1024 * 2)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        ds = ds.with_options(opt)\n",
    "\n",
    "    if labeled:\n",
    "        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    else:\n",
    "        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_ids), num_parallel_calls=AUTO)\n",
    "\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return tfds.as_numpy(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "positive-brother",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T14:25:03.685363Z",
     "iopub.status.busy": "2021-09-29T14:25:03.684685Z",
     "iopub.status.idle": "2021-09-29T14:25:03.687450Z",
     "shell.execute_reply": "2021-09-29T14:25:03.687017Z",
     "shell.execute_reply.started": "2021-09-18T04:54:42.882612Z"
    },
    "papermill": {
     "duration": 0.029534,
     "end_time": "2021-09-29T14:25:03.687553",
     "exception": false,
     "start_time": "2021-09-29T14:25:03.658019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TFRecordDataLoader:\n",
    "    def __init__(self, files, batch_size=32, cache=False, train=True, \n",
    "                              repeat=False, shuffle=False, labeled=True, \n",
    "                              return_image_ids=True):\n",
    "        self.ds = get_dataset(\n",
    "            files, \n",
    "            batch_size=batch_size,\n",
    "            cache=cache,\n",
    "            repeat=repeat,\n",
    "            shuffle=shuffle,\n",
    "            labeled=labeled,\n",
    "            return_image_ids=return_image_ids)\n",
    "        \n",
    "        self.num_examples = count_data_items(files, labeled)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.labeled = labeled\n",
    "        self.return_image_ids = return_image_ids\n",
    "        self._iterator = None\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self._iterator is None:\n",
    "            self._iterator = iter(self.ds)\n",
    "        else:\n",
    "            self._reset()\n",
    "        return self._iterator\n",
    "\n",
    "    def _reset(self):\n",
    "        self._iterator = iter(self.ds)\n",
    "\n",
    "    def __next__(self):\n",
    "        batch = next(self._iterator)\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        n_batches = self.num_examples // self.batch_size\n",
    "        if self.num_examples % self.batch_size == 0:\n",
    "            return n_batches\n",
    "        else:\n",
    "            return n_batches + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-miniature",
   "metadata": {
    "papermill": {
     "duration": 0.019231,
     "end_time": "2021-09-29T14:25:03.726002",
     "exception": false,
     "start_time": "2021-09-29T14:25:03.706771",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "significant-friendly",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T14:25:03.771517Z",
     "iopub.status.busy": "2021-09-29T14:25:03.770855Z",
     "iopub.status.idle": "2021-09-29T14:25:03.773596Z",
     "shell.execute_reply": "2021-09-29T14:25:03.773181Z",
     "shell.execute_reply.started": "2021-09-18T04:54:43.867562Z"
    },
    "papermill": {
     "duration": 0.028401,
     "end_time": "2021-09-29T14:25:03.773696",
     "exception": false,
     "start_time": "2021-09-29T14:25:03.745295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    '''\n",
    "    Code modified from the 2d code in\n",
    "    https://amaarora.github.io/2020/08/30/gempool.html\n",
    "    '''\n",
    "    def __init__(self, kernel_size=8, p=3, eps=1e-6):\n",
    "        super(GeM,self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return F.avg_pool1d(x.clamp(min=eps).pow(p), self.kernel_size).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "earlier-separate",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T14:25:03.824452Z",
     "iopub.status.busy": "2021-09-29T14:25:03.823714Z",
     "iopub.status.idle": "2021-09-29T14:25:03.826553Z",
     "shell.execute_reply": "2021-09-29T14:25:03.826128Z",
     "shell.execute_reply.started": "2021-09-18T04:54:44.958711Z"
    },
    "papermill": {
     "duration": 0.033757,
     "end_time": "2021-09-29T14:25:03.826659",
     "exception": false,
     "start_time": "2021-09-29T14:25:03.792902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN1d(nn.Module):\n",
    "    \"\"\"1D convolutional neural network. Classifier of the gravitational waves.\n",
    "    Architecture from there https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, debug=False):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(3, 64, kernel_size=64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 64, kernel_size=32),\n",
    "            GeM(kernel_size=8),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=32),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.cnn4 = nn.Sequential(\n",
    "            nn.Conv1d(128, 128, kernel_size=16),\n",
    "            GeM(kernel_size=6),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.cnn5 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=16),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.cnn6 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=16),\n",
    "            GeM(kernel_size=4),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(256 * 11, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward(self, x, pos=None):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = self.cnn4(x)\n",
    "        x = self.cnn5(x)\n",
    "        x = self.cnn6(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-postage",
   "metadata": {
    "papermill": {
     "duration": 0.019424,
     "end_time": "2021-09-29T14:25:03.865497",
     "exception": false,
     "start_time": "2021-09-29T14:25:03.846073",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bigger-relationship",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T14:25:03.912387Z",
     "iopub.status.busy": "2021-09-29T14:25:03.911662Z",
     "iopub.status.idle": "2021-09-29T14:25:03.914193Z",
     "shell.execute_reply": "2021-09-29T14:25:03.913819Z",
     "shell.execute_reply.started": "2021-09-18T04:55:16.062321Z"
    },
    "papermill": {
     "duration": 0.02918,
     "end_time": "2021-09-29T14:25:03.914316",
     "exception": false,
     "start_time": "2021-09-29T14:25:03.885136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def max_memory_allocated():\n",
    "    MB = 1024.0 * 1024.0\n",
    "    mem = torch.cuda.max_memory_allocated() / MB\n",
    "    return f\"{mem:.0f} MB\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-wallet",
   "metadata": {
    "papermill": {
     "duration": 0.020737,
     "end_time": "2021-09-29T14:25:03.955244",
     "exception": false,
     "start_time": "2021-09-29T14:25:03.934507",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "nuclear-wagner",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T14:25:04.013665Z",
     "iopub.status.busy": "2021-09-29T14:25:04.012904Z",
     "iopub.status.idle": "2021-09-29T14:25:04.015603Z",
     "shell.execute_reply": "2021-09-29T14:25:04.015160Z",
     "shell.execute_reply.started": "2021-09-18T04:55:16.548989Z"
    },
    "papermill": {
     "duration": 0.040841,
     "end_time": "2021-09-29T14:25:04.015701",
     "exception": false,
     "start_time": "2021-09-29T14:25:03.974860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(files, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    train_loader = TFRecordDataLoader(\n",
    "        files, batch_size=CFG.batch_size, \n",
    "        shuffle=True)\n",
    "    for step, d in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        x = bandpass(d[0], **CFG.bandpass_params)\n",
    "        x = torch.from_numpy(x).to(device)\n",
    "        labels = torch.from_numpy(d[1]).to(device)\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "        y_preds = model(x)\n",
    "        loss = criterion(y_preds.view(-1), labels.view(-1))\n",
    "        # record loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0:\n",
    "            print('Epoch: [{0}/{1}][{2}/{3}] '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.6f}  '\n",
    "                  'Elapsed: {remain:s} '\n",
    "                  'Max mem: {mem:s}'\n",
    "                  .format(\n",
    "                   epoch+1, CFG.epochs, step, len(train_loader),\n",
    "                   loss=losses,\n",
    "                   grad_norm=grad_norm,\n",
    "                   lr=scheduler.get_last_lr()[0],\n",
    "                   remain=timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                   mem=max_memory_allocated()))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(files, model, criterion, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    filenames = []\n",
    "    targets = []\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    valid_loader = TFRecordDataLoader(\n",
    "        files, batch_size=CFG.batch_size * 2, shuffle=False)\n",
    "    for step, d in enumerate(valid_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        targets.extend(d[1].reshape(-1).tolist())\n",
    "        filenames.extend([f.decode(\"UTF-8\") for f in d[2]])\n",
    "        x = bandpass(d[0], **CFG.bandpass_params)\n",
    "        x = torch.from_numpy(x).to(device)\n",
    "        labels = torch.from_numpy(d[1]).to(device)\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(x)\n",
    "        loss = criterion(y_preds.view(-1), labels.view(-1))\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0:\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(\n",
    "                   step, len(valid_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses,\n",
    "                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                   ))\n",
    "    predictions = np.concatenate(preds).reshape(-1)\n",
    "    return losses.avg, predictions, np.array(targets), np.array(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-theorem",
   "metadata": {
    "papermill": {
     "duration": 0.019458,
     "end_time": "2021-09-29T14:25:04.054600",
     "exception": false,
     "start_time": "2021-09-29T14:25:04.035142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dimensional-apparatus",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-29T14:25:04.109459Z",
     "iopub.status.busy": "2021-09-29T14:25:04.108684Z",
     "iopub.status.idle": "2021-09-29T14:25:04.111439Z",
     "shell.execute_reply": "2021-09-29T14:25:04.111003Z",
     "shell.execute_reply.started": "2021-09-18T04:55:17.026601Z"
    },
    "papermill": {
     "duration": 0.037375,
     "end_time": "2021-09-29T14:25:04.111538",
     "exception": false,
     "start_time": "2021-09-29T14:25:04.074163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Train loop\n",
    "# ====================================================\n",
    "def train_loop(train_tfrecords: np.ndarray, val_tfrecords: np.ndarray, fold: int):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler \n",
    "    # ====================================================\n",
    "    def get_scheduler(optimizer):\n",
    "        if CFG.scheduler=='ReduceLROnPlateau':\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                             mode='max', \n",
    "                                                             factor=CFG.factor, \n",
    "                                                             patience=CFG.patience, \n",
    "                                                             verbose=True, \n",
    "                                                             eps=CFG.eps)\n",
    "        elif CFG.scheduler=='CosineAnnealingLR':\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                             T_max=CFG.T_max, \n",
    "                                                             eta_min=CFG.min_lr, \n",
    "                                                             last_epoch=-1)\n",
    "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                                       T_0=CFG.T_0, \n",
    "                                                                       T_mult=1, \n",
    "                                                                       eta_min=CFG.min_lr, \n",
    "                                                                       last_epoch=-1)\n",
    "        return scheduler\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CNN1d()\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_score = 0.\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(\"\\n\\n\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        avg_loss = train_fn(train_tfrecords, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, preds, targets, files = valid_fn(val_tfrecords, model, criterion, device)\n",
    "        valid_result_df = pd.DataFrame({\"target\": targets, \"preds\": preds, \"id\": files})\n",
    "        \n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif isinstance(scheduler, optim.lr_scheduler.CosineAnnealingLR):\n",
    "            scheduler.step()\n",
    "        elif isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(targets, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                        'preds': preds},\n",
    "                        SAVEDIR / f'{CFG.model_name}_fold{fold}_best_score.pth')\n",
    "        \n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                        'preds': preds},\n",
    "                        SAVEDIR / f'{CFG.model_name}_fold{fold}_best_loss.pth')\n",
    "    \n",
    "    valid_result_df[\"preds\"] = torch.load(SAVEDIR / f\"{CFG.model_name}_fold{fold}_best_loss.pth\",\n",
    "                                          map_location=\"cpu\")[\"preds\"]\n",
    "\n",
    "    return valid_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "victorian-interface",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T14:25:04.161125Z",
     "iopub.status.busy": "2021-09-29T14:25:04.160269Z",
     "iopub.status.idle": "2021-09-29T17:50:05.083937Z",
     "shell.execute_reply": "2021-09-29T17:50:05.083436Z",
     "shell.execute_reply.started": "2021-09-18T04:55:17.504312Z"
    },
    "papermill": {
     "duration": 12300.953034,
     "end_time": "2021-09-29T17:50:05.084062",
     "exception": false,
     "start_time": "2021-09-29T14:25:04.131028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [1/4][0/8960] Loss: 0.7273(0.7273) Grad: 12.6823  LR: 0.000100  Elapsed: 0m 5s (remain 860m 43s) Max mem: 610 MB\n",
      "Epoch: [1/4][2500/8960] Loss: 0.5034(0.5165) Grad: 1.2830  LR: 0.000100  Elapsed: 6m 18s (remain 16m 16s) Max mem: 629 MB\n",
      "Epoch: [1/4][5000/8960] Loss: 0.5248(0.4874) Grad: 1.1754  LR: 0.000100  Elapsed: 12m 19s (remain 9m 45s) Max mem: 629 MB\n",
      "Epoch: [1/4][7500/8960] Loss: 0.4562(0.4741) Grad: 1.7776  LR: 0.000100  Elapsed: 18m 16s (remain 3m 33s) Max mem: 629 MB\n",
      "EVAL: [0/1120] Data 0.843 (0.843) Elapsed 0m 0s (remain 18m 10s) Loss: 0.4599(0.4599) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.4692  avg_val_loss: 0.4326  time: 1510s\n",
      "Epoch 1 - Score: 0.8597\n",
      "Epoch 1 - Save Best Score: 0.8597 Model\n",
      "Epoch 1 - Save Best Loss: 0.4326 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [2/4][0/8960] Loss: 0.3084(0.3084) Grad: 1.2070  LR: 0.000090  Elapsed: 0m 3s (remain 516m 32s) Max mem: 629 MB\n",
      "Epoch: [2/4][2500/8960] Loss: 0.3506(0.4345) Grad: 0.9056  LR: 0.000090  Elapsed: 6m 10s (remain 15m 55s) Max mem: 629 MB\n",
      "Epoch: [2/4][5000/8960] Loss: 0.4435(0.4317) Grad: 1.1018  LR: 0.000090  Elapsed: 12m 13s (remain 9m 40s) Max mem: 629 MB\n",
      "Epoch: [2/4][7500/8960] Loss: 0.3652(0.4292) Grad: 1.1049  LR: 0.000090  Elapsed: 18m 21s (remain 3m 34s) Max mem: 629 MB\n",
      "EVAL: [0/1120] Data 0.758 (0.758) Elapsed 0m 0s (remain 16m 41s) Loss: 0.4395(0.4395) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.4283  avg_val_loss: 0.4239  time: 1525s\n",
      "Epoch 2 - Score: 0.8650\n",
      "Epoch 2 - Save Best Score: 0.8650 Model\n",
      "Epoch 2 - Save Best Loss: 0.4239 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [3/4][0/8960] Loss: 0.4181(0.4181) Grad: 1.3157  LR: 0.000065  Elapsed: 0m 3s (remain 596m 13s) Max mem: 629 MB\n",
      "Epoch: [3/4][2500/8960] Loss: 0.4568(0.4161) Grad: 1.0315  LR: 0.000065  Elapsed: 6m 10s (remain 15m 56s) Max mem: 629 MB\n",
      "Epoch: [3/4][5000/8960] Loss: 0.3221(0.4138) Grad: 0.8476  LR: 0.000065  Elapsed: 12m 18s (remain 9m 44s) Max mem: 629 MB\n",
      "Epoch: [3/4][7500/8960] Loss: 0.3875(0.4116) Grad: 1.2725  LR: 0.000065  Elapsed: 18m 25s (remain 3m 34s) Max mem: 629 MB\n",
      "EVAL: [0/1120] Data 0.762 (0.762) Elapsed 0m 0s (remain 16m 40s) Loss: 0.4527(0.4527) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.4107  avg_val_loss: 0.4229  time: 1512s\n",
      "Epoch 3 - Score: 0.8643\n",
      "Epoch 3 - Save Best Loss: 0.4229 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [4/4][0/8960] Loss: 0.4835(0.4835) Grad: 1.3052  LR: 0.000035  Elapsed: 0m 3s (remain 595m 2s) Max mem: 629 MB\n",
      "Epoch: [4/4][2500/8960] Loss: 0.3425(0.3992) Grad: 1.0883  LR: 0.000035  Elapsed: 6m 12s (remain 16m 1s) Max mem: 629 MB\n",
      "Epoch: [4/4][5000/8960] Loss: 0.3707(0.3961) Grad: 1.5290  LR: 0.000035  Elapsed: 12m 17s (remain 9m 43s) Max mem: 629 MB\n",
      "Epoch: [4/4][7500/8960] Loss: 0.3888(0.3915) Grad: 2.4318  LR: 0.000035  Elapsed: 18m 24s (remain 3m 34s) Max mem: 629 MB\n",
      "EVAL: [0/1120] Data 1.439 (1.439) Elapsed 0m 1s (remain 31m 11s) Loss: 0.4528(0.4528) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.3888  avg_val_loss: 0.4409  time: 1519s\n",
      "Epoch 4 - Score: 0.8603\n",
      "========== fold: 1 result ==========\n",
      "Score: 0.8643\n",
      "========== fold: 3 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [1/4][0/8960] Loss: 0.7592(0.7592) Grad: 11.9553  LR: 0.000100  Elapsed: 0m 3s (remain 538m 1s) Max mem: 629 MB\n",
      "Epoch: [1/4][2500/8960] Loss: 0.3729(0.5152) Grad: 1.1719  LR: 0.000100  Elapsed: 6m 14s (remain 16m 6s) Max mem: 629 MB\n",
      "Epoch: [1/4][5000/8960] Loss: 0.3339(0.4878) Grad: 0.8163  LR: 0.000100  Elapsed: 12m 21s (remain 9m 46s) Max mem: 629 MB\n",
      "Epoch: [1/4][7500/8960] Loss: 0.3958(0.4742) Grad: 0.9210  LR: 0.000100  Elapsed: 18m 24s (remain 3m 34s) Max mem: 629 MB\n",
      "EVAL: [0/1120] Data 0.960 (0.960) Elapsed 0m 1s (remain 20m 54s) Loss: 0.4909(0.4909) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.4694  avg_val_loss: 0.4316  time: 1568s\n",
      "Epoch 1 - Score: 0.8601\n",
      "Epoch 1 - Save Best Score: 0.8601 Model\n",
      "Epoch 1 - Save Best Loss: 0.4316 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [2/4][0/8960] Loss: 0.3888(0.3888) Grad: 0.8835  LR: 0.000090  Elapsed: 0m 6s (remain 919m 50s) Max mem: 629 MB\n",
      "Epoch: [2/4][2500/8960] Loss: 0.5569(0.4353) Grad: 1.1821  LR: 0.000090  Elapsed: 6m 13s (remain 16m 4s) Max mem: 629 MB\n",
      "Epoch: [2/4][5000/8960] Loss: 0.4004(0.4317) Grad: 1.0587  LR: 0.000090  Elapsed: 12m 19s (remain 9m 45s) Max mem: 629 MB\n",
      "Epoch: [2/4][7500/8960] Loss: 0.4136(0.4289) Grad: 1.0562  LR: 0.000090  Elapsed: 18m 24s (remain 3m 34s) Max mem: 629 MB\n",
      "EVAL: [0/1120] Data 0.845 (0.845) Elapsed 0m 0s (remain 18m 20s) Loss: 0.4944(0.4944) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.4284  avg_val_loss: 0.4287  time: 1545s\n",
      "Epoch 2 - Score: 0.8650\n",
      "Epoch 2 - Save Best Score: 0.8650 Model\n",
      "Epoch 2 - Save Best Loss: 0.4287 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [3/4][0/8960] Loss: 0.3870(0.3870) Grad: 1.0351  LR: 0.000065  Elapsed: 0m 3s (remain 583m 40s) Max mem: 629 MB\n",
      "Epoch: [3/4][2500/8960] Loss: 0.4062(0.4170) Grad: 1.1410  LR: 0.000065  Elapsed: 6m 9s (remain 15m 54s) Max mem: 629 MB\n",
      "Epoch: [3/4][5000/8960] Loss: 0.3708(0.4151) Grad: 0.9070  LR: 0.000065  Elapsed: 12m 12s (remain 9m 39s) Max mem: 629 MB\n",
      "Epoch: [3/4][7500/8960] Loss: 0.3416(0.4118) Grad: 1.6197  LR: 0.000065  Elapsed: 18m 17s (remain 3m 33s) Max mem: 629 MB\n",
      "EVAL: [0/1120] Data 1.173 (1.173) Elapsed 0m 1s (remain 24m 45s) Loss: 0.4962(0.4962) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.4111  avg_val_loss: 0.4245  time: 1552s\n",
      "Epoch 3 - Score: 0.8659\n",
      "Epoch 3 - Save Best Score: 0.8659 Model\n",
      "Epoch 3 - Save Best Loss: 0.4245 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [4/4][0/8960] Loss: 0.4935(0.4935) Grad: 1.2249  LR: 0.000035  Elapsed: 0m 4s (remain 663m 54s) Max mem: 629 MB\n",
      "Epoch: [4/4][2500/8960] Loss: 0.2812(0.4009) Grad: 1.1743  LR: 0.000035  Elapsed: 6m 12s (remain 16m 2s) Max mem: 629 MB\n",
      "Epoch: [4/4][5000/8960] Loss: 0.4526(0.3978) Grad: 2.1253  LR: 0.000035  Elapsed: 12m 21s (remain 9m 47s) Max mem: 629 MB\n",
      "Epoch: [4/4][7500/8960] Loss: 0.3830(0.3920) Grad: 2.6770  LR: 0.000035  Elapsed: 18m 27s (remain 3m 35s) Max mem: 629 MB\n",
      "EVAL: [0/1120] Data 0.899 (0.899) Elapsed 0m 1s (remain 21m 20s) Loss: 0.5182(0.5182) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.3896  avg_val_loss: 0.4355  time: 1563s\n",
      "Epoch 4 - Score: 0.8624\n",
      "========== fold: 3 result ==========\n",
      "Score: 0.8659\n",
      "========== CV ==========\n",
      "Score: 0.8649\n"
     ]
    }
   ],
   "source": [
    "def get_result(result_df):\n",
    "    preds = result_df['preds'].values\n",
    "    labels = result_df[CFG.target_col].values\n",
    "    score = get_score(labels, preds)\n",
    "    LOGGER.info(f'Score: {score:<.4f}')\n",
    "\n",
    "if CFG.train:\n",
    "    # train \n",
    "    oof_df = pd.DataFrame()\n",
    "    kf = KFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "\n",
    "    folds = list(kf.split(all_files))\n",
    "    for fold in range(CFG.n_fold):\n",
    "        if fold in CFG.trn_fold:\n",
    "            trn_idx, val_idx = folds[fold]\n",
    "            train_files = all_files[trn_idx]\n",
    "            valid_files = all_files[val_idx]\n",
    "            _oof_df = train_loop(train_files, valid_files, fold)\n",
    "            oof_df = pd.concat([oof_df, _oof_df])\n",
    "            LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "            get_result(_oof_df)\n",
    "    # CV result\n",
    "    LOGGER.info(f\"========== CV ==========\")\n",
    "    get_result(oof_df)\n",
    "    # save result\n",
    "    oof_df.to_csv(SAVEDIR / 'oof_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-calendar",
   "metadata": {
    "papermill": {
     "duration": 0.040108,
     "end_time": "2021-09-29T17:50:05.164741",
     "exception": false,
     "start_time": "2021-09-29T17:50:05.124633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "necessary-composer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T17:50:05.249722Z",
     "iopub.status.busy": "2021-09-29T17:50:05.249070Z",
     "iopub.status.idle": "2021-09-29T17:50:05.281639Z",
     "shell.execute_reply": "2021-09-29T17:50:05.281167Z"
    },
    "papermill": {
     "duration": 0.07711,
     "end_time": "2021-09-29T17:50:05.281751",
     "exception": false,
     "start_time": "2021-09-29T17:50:05.204641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "states = []\n",
    "for fold  in CFG.trn_fold:\n",
    "    states.append(torch.load(os.path.join(SAVEDIR, f'{CFG.model_name}_fold{fold}_best_score.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "colonial-tobacco",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T17:50:05.368657Z",
     "iopub.status.busy": "2021-09-29T17:50:05.367909Z",
     "iopub.status.idle": "2021-09-29T17:50:06.856353Z",
     "shell.execute_reply": "2021-09-29T17:50:06.857368Z",
     "shell.execute_reply.started": "2021-09-15T14:19:43.733182Z"
    },
    "papermill": {
     "duration": 1.535373,
     "end_time": "2021-09-29T17:50:06.857590",
     "exception": false,
     "start_time": "2021-09-29T17:50:05.322217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://kds-f56f84a6d403c2466d12eed4d4afaa1fe1464a9723336d38f33ca366\n",
      "gs://kds-d482711d73bef82b2ca8c1a0bd869a564992cdd7e6997df7b372ce8e\n",
      "test_files:  10\n"
     ]
    }
   ],
   "source": [
    "gcs_paths = []\n",
    "for i, j in [(0, 4), (5, 9)]:\n",
    "    path = f\"g2net-waveform-tfrecords-test-{i}-{j}\"\n",
    "    n_trial = 0\n",
    "    while True:\n",
    "        try:\n",
    "            gcs_path = KaggleDatasets().get_gcs_path(path)\n",
    "            gcs_paths.append(gcs_path)\n",
    "            print(gcs_path)\n",
    "            break\n",
    "        except:\n",
    "            if n_trial > 10:\n",
    "                break\n",
    "            n_trial += 1\n",
    "            continue\n",
    "            \n",
    "all_files = []\n",
    "for path in gcs_paths:\n",
    "    all_files.extend(np.sort(np.array(tf.io.gfile.glob(path + \"/test*.tfrecords\"))))\n",
    "    \n",
    "print(\"test_files: \", len(all_files))\n",
    "all_files = np.array(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "greatest-flesh",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T17:50:06.950795Z",
     "iopub.status.busy": "2021-09-29T17:50:06.950086Z",
     "iopub.status.idle": "2021-09-29T18:06:18.106546Z",
     "shell.execute_reply": "2021-09-29T18:06:18.107099Z",
     "shell.execute_reply.started": "2021-09-15T15:31:55.957542Z"
    },
    "papermill": {
     "duration": 971.206211,
     "end_time": "2021-09-29T18:06:18.107310",
     "exception": false,
     "start_time": "2021-09-29T17:50:06.901099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2260it [08:48,  4.28it/s]\n",
      "2260it [07:22,  5.10it/s]\n"
     ]
    }
   ],
   "source": [
    "model= CNN1d()\n",
    "model.to(device)\n",
    "\n",
    "wave_ids = []\n",
    "probs_all = []\n",
    "\n",
    "for fold, state in enumerate(states):\n",
    "\n",
    "    model.load_state_dict(state['model'])\n",
    "    model.eval()\n",
    "    probs = []\n",
    "\n",
    "    test_loader = TFRecordDataLoader(all_files, batch_size=CFG.val_batch_size, \n",
    "                                     shuffle=False, labeled=False)\n",
    "\n",
    "    for i, d in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "        x = bandpass(d[0], **CFG.bandpass_params)\n",
    "        x = torch.from_numpy(x).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(x)\n",
    "        preds = y_preds.sigmoid().to('cpu').numpy()\n",
    "        probs.append(preds)\n",
    "\n",
    "        if fold==0: # same test loader, no need to do this the second time\n",
    "            wave_ids.append(d[1].astype('U13'))\n",
    "\n",
    "    probs = np.concatenate(probs)\n",
    "    probs_all.append(probs)\n",
    "\n",
    "probs_avg = np.asarray(probs_all).mean(axis=0).flatten()\n",
    "wave_ids = np.concatenate(wave_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "empty-species",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T18:06:20.779264Z",
     "iopub.status.busy": "2021-09-29T18:06:20.760970Z",
     "iopub.status.idle": "2021-09-29T18:06:21.486398Z",
     "shell.execute_reply": "2021-09-29T18:06:21.485892Z",
     "shell.execute_reply.started": "2021-09-15T15:36:14.629422Z"
    },
    "papermill": {
     "duration": 2.0686,
     "end_time": "2021-09-29T18:06:21.486550",
     "exception": false,
     "start_time": "2021-09-29T18:06:19.417950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'id': wave_ids, 'target': probs_avg})\n",
    "# Save test dataframe to disk\n",
    "folds = '_'.join([str(s) for s in CFG.trn_fold])\n",
    "test_df.to_csv(f'{CFG.model_name}_folds_{folds}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13303.76058,
   "end_time": "2021-09-29T18:06:24.702956",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-29T14:24:40.942376",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
